{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101, DenseNet201, VGG16\n",
    "from tensorflow.keras.preprocessing import image as tf_image\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionaries saved to 'train_simple_dict.pkl' and 'val_simple_dict.pkl'\n"
     ]
    }
   ],
   "source": [
    "# read the train.csv file\n",
    "train = pd.read_csv(os.path.join('COMP90086_2023_TLLdataset/train.csv'))\n",
    "all_right_names = train['right'].unique().tolist()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 使用train_test_split分割数据集\n",
    "train_subset, val_subset = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "def generate_random_right_with_original(left_name, original_right_name, all_right_names, num=19):\n",
    "    \"\"\"首先将原始的right文件名添加到列表,然后随机生成19个其他的right文件名\"\"\"\n",
    "    # 移除原始的right文件名，确保不会在随机选择中被再次选中\n",
    "    all_right_names = [name for name in all_right_names if name != original_right_name]\n",
    "    random_right = np.random.choice(all_right_names, num, replace=False).tolist()\n",
    "    # 将原始的right文件名加入到列表的第一个位置\n",
    "    random_right.insert(0, original_right_name)\n",
    "    return random_right\n",
    "\n",
    "val_subset['special_right'] = val_subset.apply(lambda row: generate_random_right_with_original(row['left'], row['right'], all_right_names), axis=1)\n",
    "\n",
    "train_dict_simple = dict(zip(train_subset['left'], train_subset['right']))\n",
    "val_dict_simple = dict(zip(val_subset['left'], val_subset['special_right']))\n",
    "\n",
    "# with open('train_simple_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_dict_simple, f)\n",
    "\n",
    "# with open('val_simple_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_dict_simple, f)\n",
    "\n",
    "print(\"Dictionaries saved to 'train_simple_dict.pkl' and 'val_simple_dict.pkl'\")\n",
    "\n",
    "\n",
    "\n",
    "# # -------------- Test --------------\n",
    "\n",
    "# img1 = Image.open(train_left[0])\n",
    "# img2 = Image.open(train_right[0])\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# # Display first image\n",
    "# ax[0].imshow(img1)\n",
    "# ax[0].axis('off')\n",
    "# ax[0].set_title('Image from train_left')\n",
    "\n",
    "# # Display second image\n",
    "# ax[1].imshow(img2)\n",
    "# ax[1].axis('off')\n",
    "# ax[1].set_title('Image from train_right')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to test_simple_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pickle\n",
    "\n",
    "# 定义CSV文件路径\n",
    "csv_filepath = 'COMP90086_2023_TLLdataset/test_candidates.csv'\n",
    "\n",
    "# 读取CSV内容并存储到dict中\n",
    "data_dict = {}\n",
    "with open(csv_filepath, 'r') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        # 用\"left\"列的值作为key，其余列的值作为value\n",
    "        key = row[\"left\"]\n",
    "        values = [row[col] for col in reader.fieldnames if col != \"left\"]\n",
    "        data_dict[key] = values\n",
    "\n",
    "# 保存到PKL文件中\n",
    "pkl_filepath = 'test_simple_dict.pkl'\n",
    "with open(pkl_filepath, 'wb') as pkl_file:\n",
    "    pickle.dump(data_dict, pkl_file)\n",
    "\n",
    "print(\"Data saved to\", pkl_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(os.path.join('COMP90086_2023_TLLdataset/test_candidates.csv'))\n",
    "\n",
    "# # pair the left and right\n",
    "# columns = ['c{}'.format(i) for i in range(20)]\n",
    "# test['pair'] = test.apply(lambda row: (row['left'], row[columns].tolist()), axis=1)\n",
    "\n",
    "# test_data_path = os.path.join('COMP90086_2023_TLLdataset/test')\n",
    "# test_left = test_data_path + '/left/' + test['left'] + '.jpg'\n",
    "# test_right = test_data_path + '/right/' + test['pair'].apply(lambda x: x[1][0]) + '.jpg'\n",
    "\n",
    "# # Expanding the pairs to get separate rows for each left and c0-c19 pair\n",
    "# expanded_rows = []\n",
    "# for _, row in test.iterrows():\n",
    "#     left_value = row['left']\n",
    "#     for right_value in row['pair'][1]:\n",
    "#         expanded_rows.append((left_value, right_value))\n",
    "\n",
    "# expanded_df = pd.DataFrame(expanded_rows, columns=['left', 'right'])\n",
    "\n",
    "# # Constructing image paths for the expanded rows\n",
    "# expanded_df['left_path'] = test_data_path + '/left/' + expanded_df['left'] + '.jpg'\n",
    "# expanded_df['right_path'] = test_data_path + '/right/' + expanded_df['right'] + '.jpg'\n",
    "\n",
    "# # print(expanded_df.head())\n",
    "\n",
    "# # -------------- Test --------------\n",
    "# # if the left is abm, what are the right?\n",
    "# # abm = expanded_df[expanded_df['left'] == 'abm']\n",
    "# # abm_right = abm['right_path'].tolist()\n",
    "# # print(abm_right)\n",
    "\n",
    "# # according to left, giving all the right\n",
    "# # test_left = expanded_df['left'].tolist()\n",
    "# # test_right = []\n",
    "# # for i in range(len(test_left)):\n",
    "# #     test_right.append(expanded_df[expanded_df['left'] == test_left[i]]['right_path'].tolist())\n",
    "    \n",
    "# # test_dict = dict(zip(test_left, test_right))\n",
    "\n",
    "# # with open('test_dict.pkl', 'wb') as f:\n",
    "# #     pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,      # 随机旋转的度数范围\n",
    "    width_shift_range=0.2,  # 随机水平平移的范围\n",
    "    height_shift_range=0.2, # 随机垂直平移的范围\n",
    "    shear_range=0.2,        # 剪切强度\n",
    "    zoom_range=0.2,         # 随机缩放的范围\n",
    "    horizontal_flip=True,   # 随机水平翻转\n",
    "    fill_mode='nearest'     # 输入边界以外的点的填充模式\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tf_model_and_preprocess(model_name):\n",
    "    \"\"\"\n",
    "    Returns the specified pre-trained model and its associated preprocessing function.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_name : Name of the model (resnet50, resnet18, alexnet, densenet201, etc.)\n",
    "    \n",
    "    Returns:\n",
    "    - model, preprocess_func\n",
    "    \"\"\"\n",
    "    if model_name == \"resnet50\":\n",
    "        model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "        preprocess_func = tf.keras.applications.resnet50.preprocess_input\n",
    "    elif model_name == \"resnet101\":\n",
    "        model = ResNet101(weights='imagenet', include_top=False, pooling='avg')\n",
    "        preprocess_func = tf.keras.applications.resnet.preprocess_input\n",
    "    elif model_name == \"resnet152\":\n",
    "        model = ResNet152(weights='imagenet', include_top=False, pooling='avg')\n",
    "        preprocess_func = tf.keras.applications.resnet.preprocess_input\n",
    "    elif model_name == \"densenet201\":\n",
    "        model = DenseNet201(weights='imagenet', include_top=False, pooling='avg')\n",
    "        preprocess_func = tf.keras.applications.densenet.preprocess_input\n",
    "    elif model_name == \"vgg16\":\n",
    "        model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "        preprocess_func = tf.keras.applications.vgg16.preprocess_input\n",
    "    elif model_name == \"inceptionv3\":\n",
    "        # Note: InceptionV3 expects image sizes of (299, 299)\n",
    "        model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "        preprocess_func = tf.keras.applications.inception_v3.preprocess_input\n",
    "    elif model_name == \"mobilenet\":\n",
    "        model = MobileNet(weights='imagenet', include_top=False, pooling='avg')\n",
    "        preprocess_func = tf.keras.applications.mobilenet.preprocess_input\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "    \n",
    "    return model, preprocess_func\n",
    "\n",
    "# Sample usage\n",
    "# model, preprocess_func = get_tf_model_and_preprocess(\"resnet50\")\n",
    "# features = extract_features_from_images(train_left, model, preprocess_func)  # We'll need to modify the extract function to work with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def extract_features_from_images(image_paths, model, preprocess_func, target_size=(224, 224), batch_size=32, datagen=None):\n",
    "    \"\"\"\n",
    "    Extract features from a list of images.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_paths : List of image file paths\n",
    "    - model : Pre-trained model to use for feature extraction\n",
    "    - preprocess_func : Function for preprocessing input for the model\n",
    "    - target_size : Size to resize images\n",
    "    - batch_size : Number of images to process at once\n",
    "    - datagen : ImageDataGenerator instance for data augmentation (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - features : Extracted features, list of numpy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to hold computed features\n",
    "    features = []\n",
    "    \n",
    "    # Process images in batches\n",
    "    for start_idx in range(0, len(image_paths), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(image_paths))\n",
    "        batch_image_paths = image_paths[start_idx:end_idx]\n",
    "        \n",
    "        batch_images = [img_to_array(load_img(img_path, target_size=target_size)) for img_path in batch_image_paths]\n",
    "        batch_images = np.array(batch_images)\n",
    "        \n",
    "        # Apply data augmentation if datagen is provided\n",
    "        if datagen:\n",
    "            batch_images = next(datagen.flow(batch_images, shuffle=False, batch_size=len(batch_images)))\n",
    "        \n",
    "        # Preprocess images for the model\n",
    "        batch_images_preprocessed = preprocess_func(batch_images)\n",
    "        \n",
    "        # Extract features\n",
    "        batch_features = model.predict(batch_images_preprocessed)\n",
    "        \n",
    "        # Append features to the list\n",
    "        features.extend(batch_features)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def save_features_to_disk(features, output_path):\n",
    "    \"\"\"\n",
    "    Save features to disk in pickle format.\n",
    "    \n",
    "    Parameters:\n",
    "    - features : Features to save\n",
    "    - output_path : Path to save the features\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "\n",
    "# Define the ResNet50 model for feature extraction\n",
    "feature_extractor_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pkl(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 987ms/step\n",
      "1/1 [==============================] - 1s 505ms/step\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "1/1 [==============================] - 1s 539ms/step\n",
      "1/1 [==============================] - 1s 509ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 543ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 539ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 545ms/step\n",
      "1/1 [==============================] - 1s 531ms/step\n",
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 538ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 1s 540ms/step\n",
      "1/1 [==============================] - 1s 509ms/step\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 537ms/step\n",
      "1/1 [==============================] - 1s 502ms/step\n",
      "1/1 [==============================] - 1s 529ms/step\n",
      "1/1 [==============================] - 0s 495ms/step\n",
      "1/1 [==============================] - 0s 498ms/step\n",
      "1/1 [==============================] - 1s 507ms/step\n",
      "1/1 [==============================] - 1s 502ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 502ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 537ms/step\n",
      "1/1 [==============================] - 1s 559ms/step\n",
      "1/1 [==============================] - 1s 509ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 1s 512ms/step\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 543ms/step\n",
      "1/1 [==============================] - 1s 616ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "1/1 [==============================] - 1s 509ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "1/1 [==============================] - 1s 537ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 531ms/step\n",
      "1/1 [==============================] - 1s 540ms/step\n",
      "1/1 [==============================] - 1s 529ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 1s 529ms/step\n",
      "1/1 [==============================] - 1s 539ms/step\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 549ms/step\n",
      "1/1 [==============================] - 1s 536ms/step\n",
      "1/1 [==============================] - 1s 529ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 1s 533ms/step\n",
      "1/1 [==============================] - 1s 536ms/step\n",
      "1/1 [==============================] - 1s 544ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 538ms/step\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 551ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 538ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 544ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 988ms/step\n",
      "1/1 [==============================] - 1s 949ms/step\n",
      "1/1 [==============================] - 1s 950ms/step\n",
      "1/1 [==============================] - 1s 973ms/step\n",
      "1/1 [==============================] - 1s 976ms/step\n",
      "1/1 [==============================] - 1s 953ms/step\n",
      "1/1 [==============================] - 1s 960ms/step\n",
      "1/1 [==============================] - 1s 939ms/step\n",
      "1/1 [==============================] - 1s 946ms/step\n",
      "1/1 [==============================] - 1s 949ms/step\n",
      "1/1 [==============================] - 1s 947ms/step\n",
      "1/1 [==============================] - 1s 956ms/step\n",
      "1/1 [==============================] - 1s 953ms/step\n",
      "1/1 [==============================] - 1s 932ms/step\n",
      "1/1 [==============================] - 1s 975ms/step\n",
      "1/1 [==============================] - 1s 944ms/step\n",
      "1/1 [==============================] - 1s 954ms/step\n",
      "1/1 [==============================] - 1s 1000ms/step\n",
      "1/1 [==============================] - 1s 947ms/step\n",
      "1/1 [==============================] - 1s 974ms/step\n",
      "1/1 [==============================] - 1s 977ms/step\n",
      "1/1 [==============================] - 1s 998ms/step\n",
      "1/1 [==============================] - 1s 965ms/step\n",
      "1/1 [==============================] - 1s 994ms/step\n",
      "1/1 [==============================] - 1s 983ms/step\n",
      "1/1 [==============================] - 1s 948ms/step\n",
      "1/1 [==============================] - 1s 995ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 988ms/step\n",
      "1/1 [==============================] - 1s 961ms/step\n",
      "1/1 [==============================] - 1s 1000ms/step\n",
      "1/1 [==============================] - 1s 954ms/step\n",
      "1/1 [==============================] - 1s 940ms/step\n",
      "1/1 [==============================] - 1s 959ms/step\n",
      "1/1 [==============================] - 1s 927ms/step\n",
      "1/1 [==============================] - 1s 956ms/step\n",
      "1/1 [==============================] - 1s 952ms/step\n",
      "1/1 [==============================] - 1s 951ms/step\n",
      "1/1 [==============================] - 1s 935ms/step\n",
      "1/1 [==============================] - 1s 933ms/step\n",
      "1/1 [==============================] - 1s 947ms/step\n",
      "1/1 [==============================] - 1s 932ms/step\n",
      "1/1 [==============================] - 1s 929ms/step\n",
      "1/1 [==============================] - 1s 953ms/step\n",
      "1/1 [==============================] - 1s 928ms/step\n",
      "1/1 [==============================] - 1s 922ms/step\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "1/1 [==============================] - 1s 951ms/step\n",
      "1/1 [==============================] - 1s 959ms/step\n",
      "1/1 [==============================] - 1s 949ms/step\n",
      "1/1 [==============================] - 1s 974ms/step\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "1/1 [==============================] - 1s 937ms/step\n",
      "1/1 [==============================] - 1s 985ms/step\n",
      "1/1 [==============================] - 1s 937ms/step\n",
      "1/1 [==============================] - 1s 939ms/step\n",
      "1/1 [==============================] - 1s 956ms/step\n",
      "1/1 [==============================] - 1s 969ms/step\n",
      "1/1 [==============================] - 1s 962ms/step\n",
      "1/1 [==============================] - 1s 981ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 997ms/step\n",
      "1/1 [==============================] - 1s 950ms/step\n",
      "1/1 [==============================] - 1s 954ms/step\n",
      "1/1 [==============================] - 1s 939ms/step\n",
      "1/1 [==============================] - 1s 939ms/step\n",
      "1/1 [==============================] - 1s 953ms/step\n",
      "1/1 [==============================] - 1s 978ms/step\n",
      "1/1 [==============================] - 1s 952ms/step\n",
      "1/1 [==============================] - 1s 973ms/step\n",
      "1/1 [==============================] - 1s 954ms/step\n",
      "1/1 [==============================] - 1s 955ms/step\n",
      "1/1 [==============================] - 1s 975ms/step\n",
      "1/1 [==============================] - 1s 976ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 986ms/step\n",
      "1/1 [==============================] - 1s 997ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 994ms/step\n",
      "1/1 [==============================] - 1s 981ms/step\n",
      "1/1 [==============================] - 1s 968ms/step\n",
      "1/1 [==============================] - 1s 958ms/step\n",
      "1/1 [==============================] - 1s 986ms/step\n",
      "1/1 [==============================] - 1s 979ms/step\n",
      "1/1 [==============================] - 1s 964ms/step\n",
      "1/1 [==============================] - 1s 1000ms/step\n",
      "1/1 [==============================] - 1s 953ms/step\n",
      "1/1 [==============================] - 1s 990ms/step\n",
      "1/1 [==============================] - 1s 975ms/step\n",
      "1/1 [==============================] - 1s 945ms/step\n",
      "1/1 [==============================] - 1s 979ms/step\n",
      "1/1 [==============================] - 1s 947ms/step\n",
      "1/1 [==============================] - 1s 955ms/step\n",
      "1/1 [==============================] - 1s 955ms/step\n",
      "1/1 [==============================] - 1s 974ms/step\n",
      "1/1 [==============================] - 1s 987ms/step\n",
      "1/1 [==============================] - 1s 971ms/step\n",
      "1/1 [==============================] - 1s 952ms/step\n",
      "1/1 [==============================] - 1s 974ms/step\n",
      "1/1 [==============================] - 1s 944ms/step\n",
      "1/1 [==============================] - 1s 941ms/step\n",
      "1/1 [==============================] - 1s 989ms/step\n",
      "1/1 [==============================] - 1s 967ms/step\n",
      "1/1 [==============================] - 1s 945ms/step\n",
      "1/1 [==============================] - 1s 942ms/step\n",
      "1/1 [==============================] - 1s 940ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 957ms/step\n",
      "1/1 [==============================] - 1s 979ms/step\n",
      "1/1 [==============================] - 1s 949ms/step\n",
      "1/1 [==============================] - 1s 962ms/step\n",
      "1/1 [==============================] - 1s 985ms/step\n",
      "1/1 [==============================] - 1s 958ms/step\n",
      "1/1 [==============================] - 1s 966ms/step\n",
      "1/1 [==============================] - 1s 964ms/step\n",
      "1/1 [==============================] - 1s 976ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 993ms/step\n",
      "1/1 [==============================] - 1s 977ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 964ms/step\n",
      "1/1 [==============================] - 1s 537ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 877ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m model, preprocess_func \u001b[39m=\u001b[39m get_tf_model_and_preprocess(model_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m set_name, image_paths \u001b[39min\u001b[39;00m [(\u001b[39m\"\u001b[39m\u001b[39mtrain_left\u001b[39m\u001b[39m\"\u001b[39m, train_left), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                               (\u001b[39m\"\u001b[39m\u001b[39mtrain_right\u001b[39m\u001b[39m\"\u001b[39m, train_right)]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# Extract and save features\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     extract_and_save_features(image_paths, model_name, base_output_dir, set_name, model, preprocess_func)\n",
      "\u001b[1;32m/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(model_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Extract features using the provided function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m features \u001b[39m=\u001b[39m extract_features_from_images(image_paths, model, preprocess_func)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Convert list of image paths to image filenames without extensions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m keys \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(p))[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m image_paths]\n",
      "\u001b[1;32m/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m batch_images_preprocessed \u001b[39m=\u001b[39m preprocess_func(batch_images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Extract features\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m batch_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(batch_images_preprocessed)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Append features to the list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lujiema/Documents/GitHub/CVFinalProject/final.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m features\u001b[39m.\u001b[39mextend(batch_features)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:2554\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2552\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m   2553\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2554\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_function(iterator)\n\u001b[1;32m   2555\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   2556\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    865\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_function(\u001b[39m*\u001b[39margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_and_save_features(image_paths, model_name, base_output_dir, set_name, model, preprocess_func):\n",
    "    \"\"\"\n",
    "    Extract features from images and save them to the specified output directory as a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_paths : List of paths to images\n",
    "    - model_name : Name of the model to use for feature extraction\n",
    "    - base_output_dir : Base directory to save the extracted features\n",
    "    - set_name : Name of the dataset (e.g., train_left, train_right, etc.)\n",
    "    - model : Pre-trained model to use for feature extraction\n",
    "    - preprocess_func : Function for preprocessing input for the model\n",
    "    \"\"\"\n",
    "    # Create a directory specific to the model\n",
    "    model_dir = os.path.join(base_output_dir, model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract features using the provided function\n",
    "    features = extract_features_from_images(image_paths, model, preprocess_func)\n",
    "    \n",
    "    # Convert list of image paths to image filenames without extensions\n",
    "    keys = [os.path.splitext(os.path.basename(p))[0] for p in image_paths]\n",
    "    \n",
    "    # Create a dictionary with image filenames as keys and extracted features as values\n",
    "    features_dict = dict(zip(keys, features))\n",
    "    \n",
    "    # Define the output path\n",
    "    output_path = os.path.join(model_dir, f\"{model_name}_{set_name}_features.pkl\")\n",
    "    \n",
    "    # Save features dictionary\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(features_dict, f)\n",
    "\n",
    "train_data_path = os.path.join('COMP90086_2023_TLLdataset/train')\n",
    "\n",
    "train_left = train_data_path + '/left/' + train['left'] + '.jpg'\n",
    "train_right = train_data_path + '/right/' + train['right'] + '.jpg'\n",
    "\n",
    "\n",
    "# Define paths for saving features\n",
    "base_output_dir = \"feat\"\n",
    "\n",
    "# List of models to train\n",
    "models_to_train = [\"resnet50\", \"resnet101\", \"vgg16\", \"densenet201\"]\n",
    "\n",
    "# Extract and save features for each model\n",
    "for model_name in models_to_train:\n",
    "    model, preprocess_func = get_tf_model_and_preprocess(model_name)\n",
    "    \n",
    "    for set_name, image_paths in [(\"train_left\", train_left), \n",
    "                                  (\"train_right\", train_right)]:\n",
    "        \n",
    "        # Extract and save features\n",
    "        extract_and_save_features(image_paths, model_name, base_output_dir, set_name, model, preprocess_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 910ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 531ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 1s 529ms/step\n",
      "1/1 [==============================] - 1s 523ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 1s 541ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "1/1 [==============================] - 1s 557ms/step\n",
      "1/1 [==============================] - 1s 552ms/step\n",
      "1/1 [==============================] - 1s 538ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 546ms/step\n",
      "1/1 [==============================] - 1s 512ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 547ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 545ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 523ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 523ms/step\n",
      "1/1 [==============================] - 1s 535ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 546ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 523ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 549ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 512ms/step\n",
      "1/1 [==============================] - 1s 604ms/step\n",
      "Features for test_right using resnet50 have been extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "test_data_path = os.path.join('COMP90086_2023_TLLdataset/test')\n",
    "\n",
    "# Step 1: Read right filenames from the provided CSV file\n",
    "csv_file_path = 'COMP90086_2023_TLLdataset/test_candidates.csv'  # TODO: replace with the actual path to your CSV file\n",
    "with open(csv_file_path, 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)  # skip header\n",
    "    right_filenames_from_csv = [row[1:] for row in csvreader]\n",
    "    right_filenames_from_csv = [item for sublist in right_filenames_from_csv for item in sublist]  # flatten list\n",
    "\n",
    "# Step 2: Get all filenames from the test/right directory and ensure no duplicates\n",
    "test_right_path = os.path.join(test_data_path, 'right')\n",
    "all_right_filenames = [f.split('.')[0] for f in os.listdir(test_right_path) if f.endswith('.jpg')]\n",
    "assert len(all_right_filenames) == len(set(all_right_filenames)), \"There are duplicate filenames in the test/right directory\"\n",
    "\n",
    "# Step 3: Extract features for all images in test/right directory using resnet50\n",
    "model_name = \"resnet50\"\n",
    "model, preprocess_func = get_tf_model_and_preprocess(model_name)\n",
    "\n",
    "# Corrected the function call by providing the required arguments\n",
    "extract_and_save_features([os.path.join(test_right_path, f + '.jpg') for f in all_right_filenames], \n",
    "                          model_name, \"feat_test\", \"test_right\", model, preprocess_func)\n",
    "\n",
    "print(f\"Features for test_right using {model_name} have been extracted and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 545ms/step\n",
      "1/1 [==============================] - 1s 545ms/step\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "1/1 [==============================] - 1s 547ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 1s 540ms/step\n",
      "1/1 [==============================] - 1s 536ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 1s 523ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 552ms/step\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 529ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 547ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 523ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 538ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 548ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 523ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 544ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 0s 290ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Features for test_left using resnet50 have been extracted and saved.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Get all filenames from the test/left directory and ensure no duplicates\n",
    "test_left_path = os.path.join(test_data_path, 'left')\n",
    "all_left_filenames = [f.split('.')[0] for f in os.listdir(test_left_path) if f.endswith('.jpg')]\n",
    "assert len(all_left_filenames) == len(set(all_left_filenames)), \"There are duplicate filenames in the test/left directory\"\n",
    "\n",
    "# Step 2: Extract features for all images in test/left directory using resnet50\n",
    "# Using the same model and preprocess_func as before\n",
    "extract_and_save_features([os.path.join(test_left_path, f + '.jpg') for f in all_left_filenames], \n",
    "                          model_name, \"feat_test\", \"test_left\", model, preprocess_func)\n",
    "\n",
    "message = f\"Features for test_left using {model_name} have been extracted and saved.\"\n",
    "message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mry\n",
      "xyc\n"
     ]
    }
   ],
   "source": [
    "with open('train_simple_dict.pkl', 'rb') as f:\n",
    "    train_dict = pickle.load(f)\n",
    "    \n",
    "train_left = list(train_dict.keys())\n",
    "print(train_left[0])\n",
    "\n",
    "train_right = list(train_dict.values())\n",
    "print(train_right[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yfj\n",
      "['imq', 'jsb', 'hpo', 'yqs', 'xyz', 'akf', 'afo', 'uhg', 'lby', 'pwt', 'qpe', 'bzt', 'ggd', 'vkx', 'hyg', 'olr', 'six', 'gfs', 'zmb', 'yag']\n"
     ]
    }
   ],
   "source": [
    "with open('val_simple_dict.pkl', 'rb') as f:\n",
    "    val_dict = pickle.load(f)\n",
    "    \n",
    "val_left = list(val_dict.keys())\n",
    "print(val_left[0])\n",
    "\n",
    "val_right = list(val_dict.values())\n",
    "print(val_right[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the pkl file in the feat folder\n",
    "\n",
    "with open('feat/resnet50/resnet50_train_left_features.pkl', 'rb') as f:\n",
    "    resnet50_train_left_features = pickle.load(f)\n",
    "\n",
    "with open('feat/resnet50/resnet50_train_right_features.pkl', 'rb') as f:\n",
    "    resnet50_train_right_features = pickle.load(f)\n",
    "    \n",
    "    \n",
    "len(resnet50_train_left_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6858455 , 0.4106355 , 0.07232563, ..., 0.01149769, 0.63274914,\n",
       "       1.369783  ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_train_left_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def find_most_similar(test_feature, train_features, distance_metric='euclidean'):\n",
    "    if distance_metric == 'euclidean':\n",
    "        distances = [euclidean_distance(test_feature, feature) for feature in train_features]\n",
    "    elif distance_metric == 'cosine':\n",
    "        # Convert distances to similarities\n",
    "        distances = [1 - cosine_similarity(test_feature, feature) for feature in train_features]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid distance metric\")\n",
    "    \n",
    "    # Return the index of the most similar feature\n",
    "    return np.argmin(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = sum([find_most_similar(resnet50_train_left_features[i], resnet50_train_right_features) == i \n",
    "             for i in range(len(resnet50_train_left_features))])\n",
    "\n",
    "acc = count / len(resnet50_train_left_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.066\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
